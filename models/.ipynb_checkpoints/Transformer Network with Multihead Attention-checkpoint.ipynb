{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Network with Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original transformer architecture was designed with the purpose of achieving machine translation by Vaswani et. al., so it had a distinct encoder-decoder structure. For our purposes, we applied multiheaded self-attention on the words for each song for classification, so we implemented the encoder self-attention part. For our attention mechanism, we used scaled dot-product attention and also used positional encoding like Vaswani et al. in order to take into account the positions of the words within the song lyrics. On top of that, we added a max pooling layer to reduce the dimension of the output of the transformer and a dense layer with a softmax activation to finally find the corresponding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmp/ee16a-adk/cs194-129-cmq-hw3/myenv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/tmp/ee16a-adk/cs194-129-cmq-hw3/myenv/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.utils import registry\n",
    "import keras\n",
    "from keras.engine.topology import Layer\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn import preprocessing as skpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processed song lyrics\n",
    "data = pd.read_csv('../dataset/cleaned_lyrics.csv')\n",
    "\n",
    "# English-only pre-processed song lyrics\n",
    "# data = pd.read_csv('../dataset/english_cleaned_lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genres = data['genre'].unique()\n",
    "data['genre_id'] = data.groupby(['genre']).ngroup()\n",
    "\n",
    "mappings = data[['genre', 'genre_id']].drop_duplicates()\n",
    "map_list = [(genre_id, genre) for genre, genre_id in mappings.values]\n",
    "map_list.sort()\n",
    "map_list\n",
    "\n",
    "data_subset = data[['genre_id', 'genre', 'lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy_data = data['lyrics'].values\n",
    "max_words = 30000\n",
    "\n",
    "# Create a new Tokenizer\n",
    "tokenizer = text.Tokenizer(num_words=max_words, oov_token='<UNK>')\n",
    "# Feed our song lyrics to the Tokenizer\n",
    "tokenizer.fit_on_texts(numpy_data)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "    \n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= max_words} # <= because tokenizer is 1 indexed\n",
    "tokenizer.word_index[tokenizer.oov_token] = max_words + 1\n",
    "indexed_data = tokenizer.texts_to_sequences(numpy_data)\n",
    "indexed_data = np.array(indexed_data)\n",
    "\n",
    "label_encoder = skpp.LabelEncoder()\n",
    "indexed_labels = np.array(label_encoder.fit_transform(data['genre'].values))\n",
    "\n",
    "num_test = 30000\n",
    "\n",
    "# Shuffle data before splitting off test set\n",
    "random_indexes = np.random.permutation(len(indexed_labels))\n",
    "indexed_data = indexed_data[random_indexes]\n",
    "indexed_labels = indexed_labels[random_indexes]\n",
    "\n",
    "X_train = indexed_data[:-num_test]\n",
    "y_train = indexed_labels[:-num_test]\n",
    "X_test  = indexed_data[-num_test:]\n",
    "y_test  = indexed_labels[-num_test:]\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "num_words = max_words + 2\n",
    "# Truncate and pad input sequences\n",
    "max_review_length = 600\n",
    "\n",
    "X_train_padded = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test_padded = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementation from https://github.com/Kyubyong/transformer/blob/master/modules.py\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PositionalEncoding,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(PositionalEncoding,self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        _, T, E = x.get_shape().as_list()\n",
    "        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [tf.shape(x)[0], 1])\n",
    "\n",
    "        # First part of the PE function: sin and cos argument\n",
    "        position_enc = np.array([\n",
    "            [pos / np.power(10000, 2.*i/E) for i in range(E)]\n",
    "            for pos in range(T)], dtype=np.float32)\n",
    "\n",
    "        # Second part, apply the cosine to even columns and sin to odds.\n",
    "        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "\n",
    "        # Convert to a tensor\n",
    "        lookup_table = tf.convert_to_tensor(position_enc)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n",
    "        return tf.add(outputs, x)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "# Multihead attention layer definition implemented in keras\n",
    "# Inspired from https://github.com/Kyubyong/transformer/blob/master/modules.py\n",
    "def multi_head_attention(x, i, num_heads=4):\n",
    "    E = embedding_vector_length\n",
    "    queries = Dense(E, activation='relu')(x)\n",
    "    keys = Dense(E, activation='relu')(x)\n",
    "    values = Dense(E, activation='relu')(x)\n",
    "\n",
    "    # Split and concat\n",
    "    concat = lambda x: tf.concat(tf.split(x, num_heads, axis=2), axis=0)\n",
    "    Q_ = Lambda(concat, name=\"q_reshape_{}\".format(i))(queries)\n",
    "    K_ = Lambda(concat, name=\"k_reshape_{}\".format(i))(keys)\n",
    "    V_ = Lambda(concat, name=\"v_reshape_{}\".format(i))(values)\n",
    "\n",
    "    # Multiplication\n",
    "    matmul = lambda x: tf.matmul(x[0], tf.transpose(x[1], (0, 2, 1)))\n",
    "    # permute_k = Permute((2, 1))(K_)\n",
    "    # outputs = K.batch_dot(Q_, permute_k) # (h*N, T_q, T_k)\n",
    "    outputs = Lambda(matmul, name=\"q_k_matmul_{}\".format(i))([Q_, K_])\n",
    "\n",
    "    # Scale\n",
    "    divide = lambda x: x / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "    outputs = Lambda(divide, name=\"divide_{}\".format(i))(outputs)\n",
    "\n",
    "    # Softmax\n",
    "    softmax = lambda x: tf.nn.softmax(x)\n",
    "    outputs = Lambda(softmax, name=\"softmax_{}\".format(i))(outputs)\n",
    "\n",
    "    # Dropouts\n",
    "    outputs = Dropout(0.1)(outputs)\n",
    "\n",
    "    # Weighted sum\n",
    "    matmul2 = lambda x: tf.matmul(x[0], x[1])\n",
    "    outputs = Lambda(matmul2, name=\"o_v_matmul_{}\".format(i))([outputs, V_])\n",
    "\n",
    "    # outputs = K.batch_dot(outputs, V_) # ( h*N, T_q, C/h)\n",
    "\n",
    "    # Restore shape\n",
    "    concat2 = lambda x: tf.concat(tf.split(x, num_heads, axis=0), axis=2)\n",
    "    outputs = Lambda(concat2, name=\"o_reshape_{}\".format(i))(outputs) # (N, T_q, C)\n",
    "\n",
    "    return Add()([outputs, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8983a2025b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_genres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_review_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_vector_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_review_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_vector_length = 100\n",
    "N = 6\n",
    "M = 4\n",
    "conv1_filters = 400\n",
    "conv2_filters = embedding_vector_length\n",
    "num_genres = 11\n",
    "\n",
    "# Create the Transformer model with Keras\n",
    "inputs = Input(shape=(max_review_length,))\n",
    "\n",
    "embeds = Embedding(num_words, embedding_vector_length, input_length=max_review_length)(inputs)\n",
    "transformer_input = PositionalEncoding()(embeds)\n",
    "\n",
    "# N Transformer blocks\n",
    "for i in range(N):\n",
    "    multi_head = multi_head_attention(transformer_input, i, M)\n",
    "    norm = BatchNormalization()(multi_head)\n",
    "    conv1 = Conv1D(conv1_filters, 1, activation='relu')(norm)\n",
    "    conv2 = Conv1D(conv2_filters, 1, activation=None)(conv1)\n",
    "    res = Add()([norm, conv2])\n",
    "    transformer_input = BatchNormalization()(res)\n",
    "    \n",
    "pooling = GlobalMaxPooling1D()(transformer_input)\n",
    "outputs = Dense(num_genres, activation='softmax')(pooling)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=3, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 3553s 118ms/step\n",
      "Accuracy: 49.75%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model on the test set\n",
    "scores = model.evaluate(X_test_padded, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model weights for later use, architecture cannot be saved with the use of LambdaLayers.\n",
    "model.save_weights('transformer-1-epoch-weights_1layer.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeprlbootcamp)",
   "language": "python",
   "name": "deeprlboootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
