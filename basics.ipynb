{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence, text\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn import preprocessing as skpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./dataset/cleaned_lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data)\n",
    "\n",
    "# cols: index, song, year, artist, genre, lyrics\n",
    "# N = 227449 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy_data = data['lyrics'].values\n",
    "max_words = 5000\n",
    "\n",
    "# create a new Tokenizer\n",
    "tokenizer = text.Tokenizer(num_words=max_words, oov_token='<UNK>')\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(numpy_data)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed_data = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for txt in numpy_data:\n",
    "    wordIndices = [dictionary[word] for word in text.text_to_word_sequence(txt)]\n",
    "    indexed_data.append(wordIndices)\n",
    "\n",
    "# now we have a list of all tweets converted to index arrays.\n",
    "# cast as an array for future usage.\n",
    "indexed_data = np.asarray(indexed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 61, 74, 3, 517, 3, 27, 15, 81, 534, 80, 4, 1, 1279, 105, 928, 139, 184, 24, 7, 247, 2, 25, 4, 110, 13, 2, 49, 2382, 18, 6, 967, 2581, 3, 27, 257, 87, 967, 159, 3, 3, 78, 7, 30, 14, 61, 218, 40, 210, 3, 22, 86, 4, 173, 146, 335, 100, 51, 1, 1454, 18, 205, 56, 3, 4, 195, 7, 74, 3, 78, 2, 3500, 247, 1016, 99, 6, 231, 910, 111, 125, 3, 38, 1, 1007, 4, 8, 90, 24, 3, 85, 81, 86, 10, 203, 790, 3, 322, 32, 8, 316, 5, 195, 7, 1425, 3, 285, 27, 49, 187, 28, 86, 18, 7, 4, 355, 30, 102, 231, 30, 102, 765, 30, 102, 428, 10, 127, 1083, 30, 102, 163, 30, 102, 1192, 73, 278, 25, 29, 100, 73, 37, 65, 10, 32, 73, 38, 6, 231, 2758, 488, 6, 5001, 2758, 2, 26, 118, 231, 2758, 30, 102, 163, 73, 224, 25, 29, 100, 73, 37, 65, 10, 32, 3280, 15, 3555, 80, 35, 2, 22, 931, 3, 37, 170, 23, 7, 97, 3, 89, 57, 1, 746, 105, 173, 10, 5001, 2, 173, 10, 5001, 3, 1517, 33, 3, 137, 14, 34, 15, 907, 23, 459, 2, 27, 15, 896, 3, 23, 121, 1364, 151, 556, 121, 3276, 383, 6, 1026, 30, 8, 373, 97, 269, 8, 119, 209, 3, 6, 5001, 4, 48, 415, 11, 174, 25, 7, 30, 102, 231, 30, 102, 765, 30, 102, 428, 10, 127, 1083, 30, 102, 163, 30, 102, 1192, 2, 278, 25, 29, 100, 2, 37, 65, 10, 32, 2, 38, 6, 231, 2758, 488, 6, 5001, 2758, 24, 73, 26, 8, 231, 2758, 30, 102, 163, 2, 224, 25, 29, 100, 2, 37, 65, 10, 32, 2, 2, 224, 25, 29, 100, 2, 37, 65, 10, 32, 2, 2, 278, 25, 29, 100, 2, 37, 65, 10, 32, 2, 2, 37, 65, 10, 32, 2, 37, 65, 10, 32, 2, 224, 25, 29, 100, 2, 37, 65, 10, 32, 30, 102, 231, 30, 102, 765, 30, 102, 428, 10, 127, 1083, 30, 102, 163, 30, 102, 1192, 73, 278, 25, 29, 100, 73, 37, 65, 10, 32, 73, 38, 6, 231, 2758, 488, 6, 5001, 2758, 488, 6, 5001, 2758, 2, 26, 118, 231, 2758, 30, 102, 163, 73, 224, 25, 29, 100, 73, 37, 65, 10, 32, 2758, 21, 231, 3, 268, 1641, 2, 38, 126, 462, 4, 78, 25, 15, 13, 251, 2758, 21, 428, 42, 3, 85, 27, 2, 22, 86, 28, 370, 2, 37, 302, 10, 23, 3056]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= max_words} # <= because tokenizer is 1 indexed\n",
    "tokenizer.word_index[tokenizer.oov_token] = max_words + 1\n",
    "indexed_data = tokenizer.texts_to_sequences(numpy_data)\n",
    "indexed_data = np.array(indexed_data)\n",
    "\n",
    "print(indexed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[528, 201, 82, 1383, 528, 201, 82, 1383, 528, 201, 82, 1383, 528, 201, 82, 1383, 528, 201, 82, 1383, 218, 44, 1226, 4, 1, 370, 200, 250, 32, 320, 9, 1, 208, 259, 32, 5, 741, 10, 471, 10, 954, 1, 712, 5, 113, 145, 3, 37, 36, 4, 153, 201, 141, 5, 33, 1, 445, 928, 19, 2262, 141, 138, 385, 4, 100, 6, 2626, 9, 29, 403, 309, 20, 154, 1, 248, 5001, 197, 141, 20, 38, 201, 664, 36, 20, 162, 38, 121, 16, 9, 6, 4325, 5001, 20, 38, 201, 664, 36, 20, 162, 38, 121, 16, 9, 6, 4325, 5001, 20, 38, 201, 3683, 20, 38, 201, 3683, 20, 38, 201, 3683, 20, 38, 201, 3683, 1662, 46, 2, 60, 9, 5, 2, 3002, 14, 1, 244, 5, 20, 1848, 1, 2568, 21, 2, 271, 3, 27, 20, 38, 95, 183, 14, 100, 3, 152, 27, 13, 33, 20, 40, 85, 152, 1120, 42, 3, 152, 926, 5, 437, 201, 32, 104, 10, 4, 201, 67, 121, 32, 67, 7, 36, 121, 1873, 185, 2013, 5, 67, 10, 985, 40, 12, 1085, 14, 320, 40, 334, 35, 1662, 2, 36, 34, 2, 36, 954, 5001, 68, 1, 200, 101, 926, 218, 44, 100, 3, 27, 20, 5001, 314, 5001, 314, 40, 320, 40, 40, 35, 2, 101, 48, 16, 6, 8, 200, 1694, 6, 354, 42, 142, 23, 7, 101, 48, 142, 31, 926, 23, 5001, 5, 5001, 20, 103, 4, 2428, 6, 5001, 35, 40, 12, 712, 14, 5, 8, 200, 162, 492, 7, 107, 926, 20, 38, 201, 3683, 20, 38, 201, 3683, 20, 38, 201, 3683, 20, 38, 201, 3683, 218, 528, 201, 82, 1383, 218, 528, 201, 82, 1383, 218, 528, 201, 82, 1383, 218, 528, 201, 82, 1383, 20, 38, 201, 3683, 20, 38, 201, 3683, 20, 38, 201, 3683, 20, 38, 201, 3683]\n",
      "315\n",
      "Catch Em By Surprise Catch Em By Surprise Catch Em By Surprise Catch Em By Surprise Catch em by surprise Let's go Bounce to the beat people hands up Everybody in the place stand up And jump it shake it bust the club And there's nothing you can do to hold em off And when the beautiful women be showing off We're bout to cause a riot in this house Till we hear the fire alarms going off We got em watching Do we really got them all in a trance Aoooww We got em watching Do we really got them all in a trance Aoowww We got em hypnotized We got em hypnotized We got em hypnotized We got em hypnotized Everytime time I come in And I stumble on the feeling And we lit the fires So I hope you know we got our run on Cause you gotta know that when we get Ain't gotta question if you gotta C'mon And hurt em up Give it to em Let them up Let me do them Spark another match and let it flame Get your dumb on Everybody get ready now Everytime I do what I do Bust ryhmes Make the people wanna C'mon Let's go cause you know we go'n fly Go'n fly Get Everybody get get Now I wanna see all a my people form a line If ya with me wanna see ya just C'mon With Diplo and Tiesto We about to create a fiasco Now get your club on And my people really wit me then C'mon We got em hypnotized We got em hypnotized We got em hypnotized We got em hypnotized Let's catch em by surprise Let's catch em by surprise Let's catch em by surprise Let's catch em by surprise We got em hypnotized We got em hypnotized We got em hypnotized We got em hypnotized [ \n",
      "316\n",
      "47306\n"
     ]
    }
   ],
   "source": [
    "print(indexed_data[133643])\n",
    "print(len(indexed_data[133643]))\n",
    "print(data['lyrics'][133643])\n",
    "print(len(data['lyrics'][133643].split()))\n",
    "print(dictionary['diplo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#max lyric length is 6208 at song #9467\n",
    "#top 10 lyric lengths: [5131 4287 6208 3278 3167 3155 3153 2997 2750 2660]\n",
    "#for top 1000 lengthiest songs, even first 1000 words seems sufficient\n",
    "#for top 100 lengthiest songs, first 1500 words seems sufficient\n",
    "#np.max(np.vectorize(len)(indexed_data))\n",
    "#temp = np.partition(-np.vectorize(len)(indexed_data), 100)\n",
    "#result_args = temp[:100]\n",
    "\n",
    "label_encoder = skpp.LabelEncoder()\n",
    "indexed_labels = np.array(label_encoder.fit_transform(data['genre'].values))\n",
    "#label_encoder.inverse_transform(np.array([10, 8])) #to get original genre text back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_test = 30000\n",
    "\n",
    "#shuffle data before splitting off test set\n",
    "random_indexes = np.random.permutation(len(indexed_labels))\n",
    "indexed_data = indexed_data[random_indexes]\n",
    "indexed_labels = indexed_labels[random_indexes]\n",
    "\n",
    "X_train = indexed_data[:-num_test]\n",
    "y_train = indexed_labels[:-num_test]\n",
    "X_test  = indexed_data[-num_test:]\n",
    "y_test  = indexed_labels[-num_test:]\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_words = max_words + 2\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 1000\n",
    "\n",
    "X_train_padded = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test_padded = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vocab size = 336097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1000, 32)          160064    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 11)                1111      \n",
      "=================================================================\n",
      "Total params: 214,375\n",
      "Trainable params: 214,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "  4288/197449 [..............................] - ETA: 1:22:08 - loss: 1.9135 - acc: 0.4513"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1e07d13201ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(11, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train_padded, y_train, nb_epoch=1, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
