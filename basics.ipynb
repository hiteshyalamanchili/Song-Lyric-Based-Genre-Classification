{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence, text\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn import preprocessing as skpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./dataset/cleaned_lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data)\n",
    "# cols: index, song, year, artist, genre, lyrics\n",
    "# N = 227449 songs\n",
    "#vocab size = 336097\n",
    "\n",
    "#max lyric length is 6208 at song #9467\n",
    "#top 10 lyric lengths: [5131 4287 6208 3278 3167 3155 3153 2997 2750 2660]\n",
    "#for top 1000 lengthiest songs, even first 1000 words seems sufficient\n",
    "#for top 100 lengthiest songs, first 1500 words seems sufficient\n",
    "#np.max(np.vectorize(len)(indexed_data))\n",
    "#temp = np.partition(-np.vectorize(len)(indexed_data), 100)\n",
    "#result_args = temp[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy_data = data['lyrics'].values\n",
    "max_words = 30000\n",
    "\n",
    "# create a new Tokenizer\n",
    "tokenizer = text.Tokenizer(num_words=max_words, oov_token='<UNK>')\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(numpy_data)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "    \n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= max_words} # <= because tokenizer is 1 indexed\n",
    "tokenizer.word_index[tokenizer.oov_token] = max_words + 1\n",
    "indexed_data = tokenizer.texts_to_sequences(numpy_data)\n",
    "indexed_data = np.array(indexed_data)\n",
    "\n",
    "label_encoder = skpp.LabelEncoder()\n",
    "indexed_labels = np.array(label_encoder.fit_transform(data['genre'].values))\n",
    "#label_encoder.inverse_transform(np.array([10, 8])) #to get original genre text back\n",
    "\n",
    "num_test = 30000\n",
    "\n",
    "#shuffle data before splitting off test set\n",
    "random_indexes = np.random.permutation(len(indexed_labels))\n",
    "indexed_data = indexed_data[random_indexes]\n",
    "indexed_labels = indexed_labels[random_indexes]\n",
    "\n",
    "X_train = indexed_data[:-num_test]\n",
    "y_train = indexed_labels[:-num_test]\n",
    "X_test  = indexed_data[-num_test:]\n",
    "y_test  = indexed_labels[-num_test:]\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "num_words = max_words + 2\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 1000\n",
    "\n",
    "X_train_padded = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test_padded = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#plt.hist(list(tokenizer.word_counts.values()), log=True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1000, 100)         3000200   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 1000, 60)          38640     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 11)                671       \n",
      "=================================================================\n",
      "Total params: 3,039,511\n",
      "Trainable params: 3,039,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "197449/197449 [==============================] - 6035s 31ms/step - loss: 1.3627 - acc: 0.5665\n",
      "Epoch 2/3\n",
      "197449/197449 [==============================] - 6231s 32ms/step - loss: 1.1713 - acc: 0.6248\n",
      "Epoch 3/3\n",
      "197449/197449 [==============================] - 6233s 32ms/step - loss: 1.0676 - acc: 0.6608\n",
      "Accuracy: 62.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmp/ee16a-adk/cs194-129-cmq-hw3/myenv/lib/python3.6/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vector_length = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(60, return_sequences=True))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train_padded, y_train, nb_epoch=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('lstm_attempt.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
