{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.utils import registry\n",
    "import keras\n",
    "from keras.engine.topology import Layer\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import LSTM\n",
    "# from keras.layers import GlobalMaxPooling1D\n",
    "# from keras.layers import BatchNormalization\n",
    "# from keras.layers import Lambda\n",
    "from keras.layers import *\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence, text\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn import preprocessing as skpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./dataset/cleaned_lyrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = data['genre'].unique()\n",
    "data['genre_id'] = data.groupby(['genre']).ngroup()\n",
    "\n",
    "mappings = data[['genre', 'genre_id']].drop_duplicates()\n",
    "map_list = [(genre_id, genre) for genre, genre_id in mappings.values]\n",
    "map_list.sort()\n",
    "map_list\n",
    "\n",
    "data_subset = data[['genre_id', 'genre', 'lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_data = data['lyrics'].values\n",
    "max_words = 30000\n",
    "\n",
    "# create a new Tokenizer\n",
    "tokenizer = text.Tokenizer(num_words=max_words, oov_token='<UNK>')\n",
    "# feed our song lyrics to the Tokenizer\n",
    "tokenizer.fit_on_texts(numpy_data)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "    \n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= max_words} # <= because tokenizer is 1 indexed\n",
    "tokenizer.word_index[tokenizer.oov_token] = max_words + 1\n",
    "indexed_data = tokenizer.texts_to_sequences(numpy_data)\n",
    "indexed_data = np.array(indexed_data)\n",
    "\n",
    "label_encoder = skpp.LabelEncoder()\n",
    "indexed_labels = np.array(label_encoder.fit_transform(data['genre'].values))\n",
    "#label_encoder.inverse_transform(np.array([10, 8])) #to get original genre text back\n",
    "\n",
    "num_test = 30000\n",
    "\n",
    "#shuffle data before splitting off test set\n",
    "random_indexes = np.random.permutation(len(indexed_labels))\n",
    "indexed_data = indexed_data[random_indexes]\n",
    "indexed_labels = indexed_labels[random_indexes]\n",
    "\n",
    "X_train = indexed_data[:-num_test]\n",
    "y_train = indexed_labels[:-num_test]\n",
    "X_test  = indexed_data[-num_test:]\n",
    "y_test  = indexed_labels[-num_test:]\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "num_words = max_words + 2\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 1000\n",
    "\n",
    "X_train_padded = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test_padded = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation from https://github.com/Kyubyong/transformer/blob/master/modules.py\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PositionalEncoding,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(PositionalEncoding,self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        _, T, E = x.get_shape().as_list()\n",
    "        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [tf.shape(x)[0], 1])\n",
    "\n",
    "        # First part of the PE function: sin and cos argument\n",
    "        position_enc = np.array([\n",
    "            [pos / np.power(10000, 2.*i/E) for i in range(E)]\n",
    "            for pos in range(T)], dtype=np.float32)\n",
    "\n",
    "        # Second part, apply the cosine to even columns and sin to odds.\n",
    "        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "\n",
    "        # Convert to a tensor\n",
    "        lookup_table = tf.convert_to_tensor(position_enc)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n",
    "        return tf.add(outputs, x)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def multi_head_attention(x, num_heads=10):\n",
    "    E = embedding_vector_length\n",
    "    queries = Dense(E, activation='relu')(x)\n",
    "    keys = Dense(E, activation='relu')(x)\n",
    "    values = Dense(E, activation='relu')(x)\n",
    "\n",
    "    # Split and concat\n",
    "    concat = lambda x: tf.concat(tf.split(x, num_heads, axis=2), axis=0)\n",
    "    Q_ = Lambda(concat)(queries)\n",
    "    K_ = Lambda(concat)(keys)\n",
    "    V_ = Lambda(concat)(values)\n",
    "\n",
    "    # Multiplication\n",
    "    matmul = lambda x: tf.matmul(x[0], tf.transpose(x[1], (0, 2, 1)))\n",
    "    # permute_k = Permute((2, 1))(K_)\n",
    "    # outputs = K.batch_dot(Q_, permute_k) # (h*N, T_q, T_k)\n",
    "    outputs = Lambda(matmul)([Q_, K_])\n",
    "\n",
    "    # Scale\n",
    "    divide = lambda x: x / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "    outputs = Lambda(divide)(outputs)\n",
    "\n",
    "    # Softmax\n",
    "    softmax = lambda x: tf.nn.softmax(x)\n",
    "    outputs = Lambda(softmax)(outputs)\n",
    "    # outputs = K.softmax(outputs) # (h*N, T_q, T_k)\n",
    "\n",
    "    # Dropouts\n",
    "    outputs = Dropout(0.1)(outputs)\n",
    "\n",
    "    # Weighted sum\n",
    "    matmul2 = lambda x: tf.matmul(x[0], x[1])\n",
    "    outputs = Lambda(matmul2)([outputs, V_])\n",
    "\n",
    "    # outputs = K.batch_dot(outputs, V_) # ( h*N, T_q, C/h)\n",
    "\n",
    "    # Restore shape\n",
    "    concat2 = lambda x: tf.concat(tf.split(x, num_heads, axis=0), axis=2)\n",
    "    outputs = Lambda(concat2)(outputs) # (N, T_q, C)\n",
    "\n",
    "    return Add()([outputs, x])\n",
    "\n",
    "def feed_forward(x):\n",
    "    # Inner layer\n",
    "    params = {\"inputs\": x, \"filters\": 2048, \"kernel_size\": 1,\n",
    "              \"activation\": tf.nn.relu, \"use_bias\": True}\n",
    "    outputs = tf.layers.conv1d(**params)\n",
    "    # Readout layer\n",
    "    params = {\"inputs\": outputs, \"filters\": 512, \"kernel_size\": 1,\n",
    "              \"activation\": None, \"use_bias\": True}\n",
    "    outputs = tf.layers.conv1d(**params)\n",
    "\n",
    "    # Residual connection\n",
    "    outputs += inputs\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_102 (InputLayer)          (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_116 (Embedding)       (None, 1000, 100)    3000200     input_102[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "positional_encoding_116 (Positi (None, 1000, 100)    0           embedding_116[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_357 (Dense)               (None, 1000, 100)    10100       positional_encoding_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_358 (Dense)               (None, 1000, 100)    10100       positional_encoding_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_571 (Lambda)             (None, 1000, 10)     0           dense_357[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_572 (Lambda)             (None, 1000, 10)     0           dense_358[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_574 (Lambda)             (None, 1000, 1000)   0           lambda_571[0][0]                 \n",
      "                                                                 lambda_572[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_575 (Lambda)             (None, 1000, 1000)   0           lambda_574[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_576 (Lambda)             (None, 1000, 1000)   0           lambda_575[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_359 (Dense)               (None, 1000, 100)    10100       positional_encoding_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)            (None, 1000, 1000)   0           lambda_576[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_573 (Lambda)             (None, 1000, 10)     0           dense_359[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_577 (Lambda)             (None, 1000, 10)     0           dropout_91[0][0]                 \n",
      "                                                                 lambda_573[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_578 (Lambda)             (None, 1000, 100)    0           lambda_577[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_89 (Add)                    (None, 1000, 100)    0           lambda_578[0][0]                 \n",
      "                                                                 positional_encoding_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 1000, 100)    400         add_89[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, 1000, 400)    40400       batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, 1000, 100)    40100       conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_90 (Add)                    (None, 1000, 100)    0           batch_normalization_70[0][0]     \n",
      "                                                                 conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 1000, 100)    400         add_90[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_360 (Dense)               (None, 1000, 100)    10100       batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_361 (Dense)               (None, 1000, 100)    10100       batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_579 (Lambda)             (None, 1000, 10)     0           dense_360[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_580 (Lambda)             (None, 1000, 10)     0           dense_361[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_582 (Lambda)             (None, 1000, 1000)   0           lambda_579[0][0]                 \n",
      "                                                                 lambda_580[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_583 (Lambda)             (None, 1000, 1000)   0           lambda_582[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_584 (Lambda)             (None, 1000, 1000)   0           lambda_583[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_362 (Dense)               (None, 1000, 100)    10100       batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)            (None, 1000, 1000)   0           lambda_584[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_581 (Lambda)             (None, 1000, 10)     0           dense_362[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_585 (Lambda)             (None, 1000, 10)     0           dropout_92[0][0]                 \n",
      "                                                                 lambda_581[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_586 (Lambda)             (None, 1000, 100)    0           lambda_585[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_91 (Add)                    (None, 1000, 100)    0           lambda_586[0][0]                 \n",
      "                                                                 batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 1000, 100)    400         add_91[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, 1000, 400)    40400       batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, 1000, 100)    40100       conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_92 (Add)                    (None, 1000, 100)    0           batch_normalization_72[0][0]     \n",
      "                                                                 conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 1000, 100)    400         add_92[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_363 (Dense)               (None, 1000, 100)    10100       batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_364 (Dense)               (None, 1000, 100)    10100       batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_587 (Lambda)             (None, 1000, 10)     0           dense_363[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_588 (Lambda)             (None, 1000, 10)     0           dense_364[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_590 (Lambda)             (None, 1000, 1000)   0           lambda_587[0][0]                 \n",
      "                                                                 lambda_588[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_591 (Lambda)             (None, 1000, 1000)   0           lambda_590[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_592 (Lambda)             (None, 1000, 1000)   0           lambda_591[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_365 (Dense)               (None, 1000, 100)    10100       batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)            (None, 1000, 1000)   0           lambda_592[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_589 (Lambda)             (None, 1000, 10)     0           dense_365[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_593 (Lambda)             (None, 1000, 10)     0           dropout_93[0][0]                 \n",
      "                                                                 lambda_589[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_594 (Lambda)             (None, 1000, 100)    0           lambda_593[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_93 (Add)                    (None, 1000, 100)    0           lambda_594[0][0]                 \n",
      "                                                                 batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 1000, 100)    400         add_93[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, 1000, 400)    40400       batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)              (None, 1000, 100)    40100       conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_94 (Add)                    (None, 1000, 100)    0           batch_normalization_74[0][0]     \n",
      "                                                                 conv1d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 1000, 100)    400         add_94[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_366 (Dense)               (None, 1000, 100)    10100       batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_367 (Dense)               (None, 1000, 100)    10100       batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_595 (Lambda)             (None, 1000, 10)     0           dense_366[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_596 (Lambda)             (None, 1000, 10)     0           dense_367[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_598 (Lambda)             (None, 1000, 1000)   0           lambda_595[0][0]                 \n",
      "                                                                 lambda_596[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_599 (Lambda)             (None, 1000, 1000)   0           lambda_598[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_600 (Lambda)             (None, 1000, 1000)   0           lambda_599[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_368 (Dense)               (None, 1000, 100)    10100       batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)            (None, 1000, 1000)   0           lambda_600[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_597 (Lambda)             (None, 1000, 10)     0           dense_368[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_601 (Lambda)             (None, 1000, 10)     0           dropout_94[0][0]                 \n",
      "                                                                 lambda_597[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_602 (Lambda)             (None, 1000, 100)    0           lambda_601[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_95 (Add)                    (None, 1000, 100)    0           lambda_602[0][0]                 \n",
      "                                                                 batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 1000, 100)    400         add_95[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_71 (Conv1D)              (None, 1000, 400)    40400       batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_72 (Conv1D)              (None, 1000, 100)    40100       conv1d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_96 (Add)                    (None, 1000, 100)    0           batch_normalization_76[0][0]     \n",
      "                                                                 conv1d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 1000, 100)    400         add_96[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_369 (Dense)               (None, 1000, 100)    10100       batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_370 (Dense)               (None, 1000, 100)    10100       batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_603 (Lambda)             (None, 1000, 10)     0           dense_369[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_604 (Lambda)             (None, 1000, 10)     0           dense_370[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_606 (Lambda)             (None, 1000, 1000)   0           lambda_603[0][0]                 \n",
      "                                                                 lambda_604[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_607 (Lambda)             (None, 1000, 1000)   0           lambda_606[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_608 (Lambda)             (None, 1000, 1000)   0           lambda_607[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_371 (Dense)               (None, 1000, 100)    10100       batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)            (None, 1000, 1000)   0           lambda_608[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_605 (Lambda)             (None, 1000, 10)     0           dense_371[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_609 (Lambda)             (None, 1000, 10)     0           dropout_95[0][0]                 \n",
      "                                                                 lambda_605[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_610 (Lambda)             (None, 1000, 100)    0           lambda_609[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_97 (Add)                    (None, 1000, 100)    0           lambda_610[0][0]                 \n",
      "                                                                 batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 1000, 100)    400         add_97[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_73 (Conv1D)              (None, 1000, 400)    40400       batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_74 (Conv1D)              (None, 1000, 100)    40100       conv1d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_98 (Add)                    (None, 1000, 100)    0           batch_normalization_78[0][0]     \n",
      "                                                                 conv1d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 1000, 100)    400         add_98[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_372 (Dense)               (None, 1000, 100)    10100       batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_373 (Dense)               (None, 1000, 100)    10100       batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_611 (Lambda)             (None, 1000, 10)     0           dense_372[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_612 (Lambda)             (None, 1000, 10)     0           dense_373[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_614 (Lambda)             (None, 1000, 1000)   0           lambda_611[0][0]                 \n",
      "                                                                 lambda_612[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_615 (Lambda)             (None, 1000, 1000)   0           lambda_614[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_616 (Lambda)             (None, 1000, 1000)   0           lambda_615[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_374 (Dense)               (None, 1000, 100)    10100       batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)            (None, 1000, 1000)   0           lambda_616[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_613 (Lambda)             (None, 1000, 10)     0           dense_374[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_617 (Lambda)             (None, 1000, 10)     0           dropout_96[0][0]                 \n",
      "                                                                 lambda_613[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_618 (Lambda)             (None, 1000, 100)    0           lambda_617[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_99 (Add)                    (None, 1000, 100)    0           lambda_618[0][0]                 \n",
      "                                                                 batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 1000, 100)    400         add_99[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_75 (Conv1D)              (None, 1000, 400)    40400       batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_76 (Conv1D)              (None, 1000, 100)    40100       conv1d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_100 (Add)                   (None, 1000, 100)    0           batch_normalization_80[0][0]     \n",
      "                                                                 conv1d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 1000, 100)    400         add_100[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 100)          0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_375 (Dense)               (None, 11)           1111        global_max_pooling1d_3[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 3,670,911\n",
      "Trainable params: 3,668,511\n",
      "Non-trainable params: 2,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[640,1000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lambda_574/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_571/concat, lambda_574/transpose)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training/Adam/gradients/lambda_600/Reshape_1_grad/Reshape/_4323 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6815_training/Adam/gradients/lambda_600/Reshape_1_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'lambda_574/MatMul', defined at:\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-194-f12a92659847>\", line 8, in <module>\n    multi_head = multi_head_attention(transformer_input)\n  File \"<ipython-input-182-af0612a55eb7>\", line 49, in multi_head_attention\n    outputs = Lambda(matmul)([Q_, K_])\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\keras\\layers\\core.py\", line 663, in call\n    return self.function(inputs, **arguments)\n  File \"<ipython-input-182-af0612a55eb7>\", line 46, in <lambda>\n    matmul = lambda x: tf.matmul(x[0], tf.transpose(x[1], (0, 2, 1)))\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2071, in matmul\n    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1295, in batch_mat_mul\n    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[640,1000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lambda_574/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_571/concat, lambda_574/transpose)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training/Adam/gradients/lambda_600/Reshape_1_grad/Reshape/_4323 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6815_training/Adam/gradients/lambda_600/Reshape_1_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[640,1000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lambda_574/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_571/concat, lambda_574/transpose)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training/Adam/gradients/lambda_600/Reshape_1_grad/Reshape/_4323 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6815_training/Adam/gradients/lambda_600/Reshape_1_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-f12a92659847>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;31m# # Final evaluation of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# scores = model.evaluate(X_test_padded, y_test, verbose=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[640,1000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lambda_574/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_571/concat, lambda_574/transpose)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training/Adam/gradients/lambda_600/Reshape_1_grad/Reshape/_4323 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6815_training/Adam/gradients/lambda_600/Reshape_1_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'lambda_574/MatMul', defined at:\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-194-f12a92659847>\", line 8, in <module>\n    multi_head = multi_head_attention(transformer_input)\n  File \"<ipython-input-182-af0612a55eb7>\", line 49, in multi_head_attention\n    outputs = Lambda(matmul)([Q_, K_])\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\keras\\layers\\core.py\", line 663, in call\n    return self.function(inputs, **arguments)\n  File \"<ipython-input-182-af0612a55eb7>\", line 46, in <lambda>\n    matmul = lambda x: tf.matmul(x[0], tf.transpose(x[1], (0, 2, 1)))\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2071, in matmul\n    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1295, in batch_mat_mul\n    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Sayan\\AppData\\Local\\conda\\conda\\envs\\cs194project\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[640,1000,1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lambda_574/MatMul = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lambda_571/concat, lambda_574/transpose)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training/Adam/gradients/lambda_600/Reshape_1_grad/Reshape/_4323 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6815_training/Adam/gradients/lambda_600/Reshape_1_grad/Reshape\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 100\n",
    "\n",
    "inputs = Input(shape=(max_review_length,))\n",
    "\n",
    "embeds = Embedding(num_words, embedding_vector_length, input_length=max_review_length)(inputs)\n",
    "transformer_input = PositionalEncoding()(embeds)\n",
    "for i in range(6):\n",
    "    multi_head = multi_head_attention(transformer_input)\n",
    "    norm = BatchNormalization()(multi_head)\n",
    "    conv1 = Conv1D(400, 1, activation='relu')(norm)\n",
    "    conv2 = Conv1D(100, 1)(conv1)\n",
    "    res = Add()([norm, conv2])\n",
    "    transformer_input = BatchNormalization()(res)\n",
    "pooling = GlobalMaxPooling1D()(transformer_input)\n",
    "outputs = Dense(11, activation='softmax')(pooling)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train_padded, y_train, nb_epoch=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
